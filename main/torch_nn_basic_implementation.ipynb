{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87ad48a-8228-4b60-bd58-0b2c164c2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorly as tl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import QuantLib as ql\n",
    "import pysabr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05cf25a0-6bfc-4b50-af50-61c2e34166a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2efe9866-c5f0-4893-915e-85f642f4ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensorly.tenalg import inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d1e0df1-2881-497f-8cee-b5fb0d24f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.set_backend('pytorch')\n",
    "batch_size = 10000\n",
    "device = 'cpu'\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "\n",
    "sabr_params = [\"Strike\", \"Tenor\", \"Alpha\", \"Beta\", \"Nu\", \"Rho\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19320c42-a1c5-4cf9-955d-cf2ea1b5f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "\n",
    "raw_data = pd.read_csv('fdm_hagan_vol_train.csv')\n",
    "test_raw_data = pd.read_csv('fdm_hagan_vol_test.csv')\n",
    "raw_data[\"Error\"] = (raw_data[\"FDM_vol\"] - raw_data[\"Hagan_vol\"])\n",
    "test_raw_data[\"Error\"] = (test_raw_data[\"FDM_vol\"] - test_raw_data[\"Hagan_vol\"])\n",
    "\n",
    "X = raw_data.drop([\"FDM_vol\", \"Hagan_vol\", \"Error\", \"Strike\", \"Nu\"], axis=1).values\n",
    "y = raw_data[\"Error\"].values\n",
    "X_test = test_raw_data.drop([\"FDM_vol\", \"Hagan_vol\", \"Error\"], axis=1).values\n",
    "y_test = test_raw_data[\"Error\"].values\n",
    "\n",
    "# scaler.fit(X)\n",
    "# X = scaler.transform(X)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba1146ca-a95f-4201-acd2-0bab785f848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(X)\n",
    "y = torch.FloatTensor(y)\n",
    "y = torch.reshape(y, shape=(-1,1))\n",
    "\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "y_test = torch.reshape(y_test, shape=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6347f776-3843-42b6-8d04-92bb39fd9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X, y)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79b310-a20c-46d1-a9ef-0ddf0aa1a093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HaganDiffNNModel(train_loader, test_loader):\n",
    "    def __init__(self):\n",
    "        super(HaganDiffNNModel, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(6, 20, device=device),\n",
    "                              nn.Linear(20,100),\n",
    "                              nn.Linear(100,400),\n",
    "                              nn.Sigmoid(),\n",
    "                              nn.Linear(400,10),           \n",
    "                              nn.Tanh(),\n",
    "                              nn.Linear(10,1),\n",
    "                              nn.Tanh()\n",
    "        )\n",
    "        self.optimizer = optim.Adam(model.parameters(),lr=0.00001)\n",
    "        self.n_epochs = 2000\n",
    "        self.batch_size = 64\n",
    "    \n",
    "    def hagan(self, X):\n",
    "        return np.array([ql.sabrVolatility(row[0], 1, *row[1:]) for row in X])\n",
    "    \n",
    "    def train(n_epoch):\n",
    "        self.model.train()\n",
    "        for i, (features, error) in enumerate(train_loader):\n",
    "            features, error = features.to(device), error.to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = model(features)\n",
    "            loss = torch.nn.functional.mse_loss(output,error)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if i % 1000 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    self.n_epoch, i * len(features), len(train_loader.dataset),\n",
    "                    100. * i / len(train_loader), loss))\n",
    "    \n",
    "    def test():\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for features, error in test_loader:\n",
    "            features, error = features.to(device), error.to(device)\n",
    "            output = self.model(features)\n",
    "            test_loss = torch.nn.functional.mse_loss(output,error)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(error.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('mean: {}'.format(test_loss))\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "           100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "    #     x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "    #     x = self.trl(x)\n",
    "    #     return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = HaganDiffNNModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "834ecfb2-3c57-46fc-8937-bf3e0ac2bde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.zero_grad of Sequential(\n",
      "  (0): Linear(in_features=6, out_features=20, bias=True)\n",
      "  (1): Linear(in_features=20, out_features=100, bias=True)\n",
      "  (2): Linear(in_features=100, out_features=400, bias=True)\n",
      "  (3): Sigmoid()\n",
      "  (4): Linear(in_features=400, out_features=10, bias=True)\n",
      "  (5): Tanh()\n",
      "  (6): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (7): Tanh()\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(6, 20, device=device),\n",
    "                      nn.Linear(20,100),\n",
    "                      nn.Linear(100,400),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(400,10),           \n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(10,1),\n",
    "                      nn.Tanh()\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "print(model.zero_grad)\n",
    "n_epochs = 2000\n",
    "# batch_size = 64\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0fec60-d8a5-475d-aaaa-c467c13a6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for i, (features, error) in enumerate(train_loader):\n",
    "        features, error = features.to(device), error.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = torch.nn.functional.mse_loss(output,error)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(features), len(train_loader.dataset),\n",
    "                100. * i / len(train_loader), loss))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for features, error in test_loader:\n",
    "        features, error = features.to(device), error.to(device)\n",
    "        output = model(features)\n",
    "        test_loss = torch.nn.functional.mse_loss(output,error)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(error.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('mean: {}'.format(test_loss))\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "       100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39a6a4e4-3ec6-40c9-91f8-d0031687e4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/177775 (0%)]\tLoss: 0.004812\n",
      "Train Epoch: 2 [0/177775 (0%)]\tLoss: 0.003340\n",
      "Train Epoch: 3 [0/177775 (0%)]\tLoss: 0.001653\n",
      "Train Epoch: 4 [0/177775 (0%)]\tLoss: 0.001737\n",
      "Train Epoch: 5 [0/177775 (0%)]\tLoss: 0.001547\n",
      "Train Epoch: 6 [0/177775 (0%)]\tLoss: 0.001585\n",
      "Train Epoch: 7 [0/177775 (0%)]\tLoss: 0.001587\n",
      "Train Epoch: 8 [0/177775 (0%)]\tLoss: 0.001550\n",
      "Train Epoch: 9 [0/177775 (0%)]\tLoss: 0.001580\n",
      "Train Epoch: 10 [0/177775 (0%)]\tLoss: 0.001565\n",
      "Train Epoch: 11 [0/177775 (0%)]\tLoss: 0.001535\n",
      "Train Epoch: 12 [0/177775 (0%)]\tLoss: 0.001570\n",
      "Train Epoch: 13 [0/177775 (0%)]\tLoss: 0.001535\n",
      "Train Epoch: 14 [0/177775 (0%)]\tLoss: 0.001501\n",
      "Train Epoch: 15 [0/177775 (0%)]\tLoss: 0.001483\n",
      "Train Epoch: 16 [0/177775 (0%)]\tLoss: 0.001460\n",
      "Train Epoch: 17 [0/177775 (0%)]\tLoss: 0.001435\n",
      "Train Epoch: 18 [0/177775 (0%)]\tLoss: 0.001336\n",
      "Train Epoch: 19 [0/177775 (0%)]\tLoss: 0.001286\n",
      "Train Epoch: 20 [0/177775 (0%)]\tLoss: 0.001168\n",
      "Train Epoch: 21 [0/177775 (0%)]\tLoss: 0.001050\n",
      "Train Epoch: 22 [0/177775 (0%)]\tLoss: 0.000864\n",
      "Train Epoch: 23 [0/177775 (0%)]\tLoss: 0.000677\n",
      "Train Epoch: 24 [0/177775 (0%)]\tLoss: 0.000497\n",
      "Train Epoch: 25 [0/177775 (0%)]\tLoss: 0.000469\n",
      "Train Epoch: 26 [0/177775 (0%)]\tLoss: 0.000294\n",
      "Train Epoch: 27 [0/177775 (0%)]\tLoss: 0.000275\n",
      "Train Epoch: 28 [0/177775 (0%)]\tLoss: 0.000271\n",
      "Train Epoch: 29 [0/177775 (0%)]\tLoss: 0.000274\n",
      "Train Epoch: 30 [0/177775 (0%)]\tLoss: 0.000258\n",
      "Train Epoch: 31 [0/177775 (0%)]\tLoss: 0.000257\n",
      "Train Epoch: 32 [0/177775 (0%)]\tLoss: 0.000300\n",
      "Train Epoch: 33 [0/177775 (0%)]\tLoss: 0.000263\n",
      "Train Epoch: 34 [0/177775 (0%)]\tLoss: 0.000255\n",
      "Train Epoch: 35 [0/177775 (0%)]\tLoss: 0.000289\n",
      "Train Epoch: 36 [0/177775 (0%)]\tLoss: 0.000254\n",
      "Train Epoch: 37 [0/177775 (0%)]\tLoss: 0.000244\n",
      "Train Epoch: 38 [0/177775 (0%)]\tLoss: 0.000295\n",
      "Train Epoch: 39 [0/177775 (0%)]\tLoss: 0.000246\n",
      "Train Epoch: 40 [0/177775 (0%)]\tLoss: 0.000262\n",
      "Train Epoch: 41 [0/177775 (0%)]\tLoss: 0.000249\n",
      "Train Epoch: 42 [0/177775 (0%)]\tLoss: 0.000264\n",
      "Train Epoch: 43 [0/177775 (0%)]\tLoss: 0.000296\n",
      "Train Epoch: 44 [0/177775 (0%)]\tLoss: 0.000262\n",
      "Train Epoch: 45 [0/177775 (0%)]\tLoss: 0.000255\n",
      "Train Epoch: 46 [0/177775 (0%)]\tLoss: 0.000246\n",
      "Train Epoch: 47 [0/177775 (0%)]\tLoss: 0.000258\n",
      "Train Epoch: 48 [0/177775 (0%)]\tLoss: 0.000251\n",
      "Train Epoch: 49 [0/177775 (0%)]\tLoss: 0.000277\n",
      "Train Epoch: 50 [0/177775 (0%)]\tLoss: 0.000287\n",
      "Train Epoch: 51 [0/177775 (0%)]\tLoss: 0.000271\n",
      "Train Epoch: 52 [0/177775 (0%)]\tLoss: 0.000262\n",
      "Train Epoch: 53 [0/177775 (0%)]\tLoss: 0.000244\n",
      "Train Epoch: 54 [0/177775 (0%)]\tLoss: 0.000305\n",
      "Train Epoch: 55 [0/177775 (0%)]\tLoss: 0.000293\n",
      "Train Epoch: 56 [0/177775 (0%)]\tLoss: 0.000246\n",
      "Train Epoch: 57 [0/177775 (0%)]\tLoss: 0.000332\n",
      "Train Epoch: 58 [0/177775 (0%)]\tLoss: 0.000330\n",
      "Train Epoch: 59 [0/177775 (0%)]\tLoss: 0.000257\n",
      "Train Epoch: 60 [0/177775 (0%)]\tLoss: 0.000242\n",
      "Train Epoch: 61 [0/177775 (0%)]\tLoss: 0.000256\n",
      "Train Epoch: 62 [0/177775 (0%)]\tLoss: 0.000330\n",
      "Train Epoch: 63 [0/177775 (0%)]\tLoss: 0.000256\n",
      "Train Epoch: 64 [0/177775 (0%)]\tLoss: 0.000278\n",
      "Train Epoch: 65 [0/177775 (0%)]\tLoss: 0.000315\n",
      "Train Epoch: 66 [0/177775 (0%)]\tLoss: 0.000233\n",
      "Train Epoch: 67 [0/177775 (0%)]\tLoss: 0.000259\n",
      "Train Epoch: 68 [0/177775 (0%)]\tLoss: 0.000239\n",
      "Train Epoch: 69 [0/177775 (0%)]\tLoss: 0.000493\n",
      "Train Epoch: 70 [0/177775 (0%)]\tLoss: 0.000236\n",
      "Train Epoch: 71 [0/177775 (0%)]\tLoss: 0.000246\n",
      "Train Epoch: 72 [0/177775 (0%)]\tLoss: 0.000255\n",
      "Train Epoch: 73 [0/177775 (0%)]\tLoss: 0.000241\n",
      "Train Epoch: 74 [0/177775 (0%)]\tLoss: 0.000271\n",
      "Train Epoch: 75 [0/177775 (0%)]\tLoss: 0.000273\n",
      "Train Epoch: 76 [0/177775 (0%)]\tLoss: 0.000249\n",
      "Train Epoch: 77 [0/177775 (0%)]\tLoss: 0.000256\n",
      "Train Epoch: 78 [0/177775 (0%)]\tLoss: 0.000237\n",
      "Train Epoch: 79 [0/177775 (0%)]\tLoss: 0.000239\n",
      "Train Epoch: 80 [0/177775 (0%)]\tLoss: 0.000234\n",
      "Train Epoch: 81 [0/177775 (0%)]\tLoss: 0.000256\n",
      "Train Epoch: 82 [0/177775 (0%)]\tLoss: 0.000261\n",
      "Train Epoch: 83 [0/177775 (0%)]\tLoss: 0.000232\n",
      "Train Epoch: 84 [0/177775 (0%)]\tLoss: 0.000295\n",
      "Train Epoch: 85 [0/177775 (0%)]\tLoss: 0.000260\n",
      "Train Epoch: 86 [0/177775 (0%)]\tLoss: 0.000275\n",
      "Train Epoch: 87 [0/177775 (0%)]\tLoss: 0.000261\n",
      "Train Epoch: 88 [0/177775 (0%)]\tLoss: 0.000249\n",
      "Train Epoch: 89 [0/177775 (0%)]\tLoss: 0.000237\n",
      "Train Epoch: 90 [0/177775 (0%)]\tLoss: 0.000274\n",
      "Train Epoch: 91 [0/177775 (0%)]\tLoss: 0.000236\n",
      "Train Epoch: 92 [0/177775 (0%)]\tLoss: 0.000233\n",
      "Train Epoch: 93 [0/177775 (0%)]\tLoss: 0.000386\n",
      "Train Epoch: 94 [0/177775 (0%)]\tLoss: 0.000270\n",
      "Train Epoch: 95 [0/177775 (0%)]\tLoss: 0.000237\n",
      "Train Epoch: 96 [0/177775 (0%)]\tLoss: 0.000235\n",
      "Train Epoch: 97 [0/177775 (0%)]\tLoss: 0.000285\n",
      "Train Epoch: 98 [0/177775 (0%)]\tLoss: 0.000228\n",
      "Train Epoch: 99 [0/177775 (0%)]\tLoss: 0.000251\n",
      "Train Epoch: 100 [0/177775 (0%)]\tLoss: 0.000275\n",
      "Train Epoch: 101 [0/177775 (0%)]\tLoss: 0.000258\n",
      "Train Epoch: 102 [0/177775 (0%)]\tLoss: 0.000235\n",
      "Train Epoch: 103 [0/177775 (0%)]\tLoss: 0.000236\n",
      "Train Epoch: 104 [0/177775 (0%)]\tLoss: 0.000262\n",
      "Train Epoch: 105 [0/177775 (0%)]\tLoss: 0.000257\n",
      "Train Epoch: 106 [0/177775 (0%)]\tLoss: 0.000265\n",
      "Train Epoch: 107 [0/177775 (0%)]\tLoss: 0.000251\n",
      "Train Epoch: 108 [0/177775 (0%)]\tLoss: 0.000233\n",
      "Train Epoch: 109 [0/177775 (0%)]\tLoss: 0.000254\n",
      "Train Epoch: 110 [0/177775 (0%)]\tLoss: 0.000572\n",
      "Train Epoch: 111 [0/177775 (0%)]\tLoss: 0.000264\n",
      "Train Epoch: 112 [0/177775 (0%)]\tLoss: 0.000222\n",
      "Train Epoch: 113 [0/177775 (0%)]\tLoss: 0.000223\n",
      "Train Epoch: 114 [0/177775 (0%)]\tLoss: 0.000222\n",
      "Train Epoch: 115 [0/177775 (0%)]\tLoss: 0.000262\n",
      "Train Epoch: 116 [0/177775 (0%)]\tLoss: 0.000268\n",
      "Train Epoch: 117 [0/177775 (0%)]\tLoss: 0.000241\n",
      "Train Epoch: 118 [0/177775 (0%)]\tLoss: 0.000231\n",
      "Train Epoch: 119 [0/177775 (0%)]\tLoss: 0.000231\n",
      "Train Epoch: 120 [0/177775 (0%)]\tLoss: 0.000310\n",
      "Train Epoch: 121 [0/177775 (0%)]\tLoss: 0.000377\n",
      "Train Epoch: 122 [0/177775 (0%)]\tLoss: 0.000251\n",
      "Train Epoch: 123 [0/177775 (0%)]\tLoss: 0.000240\n",
      "Train Epoch: 124 [0/177775 (0%)]\tLoss: 0.000231\n",
      "Train Epoch: 125 [0/177775 (0%)]\tLoss: 0.000236\n",
      "Train Epoch: 126 [0/177775 (0%)]\tLoss: 0.000225\n",
      "Train Epoch: 127 [0/177775 (0%)]\tLoss: 0.000228\n",
      "Train Epoch: 128 [0/177775 (0%)]\tLoss: 0.000224\n",
      "Train Epoch: 129 [0/177775 (0%)]\tLoss: 0.000222\n",
      "Train Epoch: 130 [0/177775 (0%)]\tLoss: 0.000231\n",
      "Train Epoch: 131 [0/177775 (0%)]\tLoss: 0.000229\n",
      "Train Epoch: 132 [0/177775 (0%)]\tLoss: 0.000226\n",
      "Train Epoch: 133 [0/177775 (0%)]\tLoss: 0.000244\n",
      "Train Epoch: 134 [0/177775 (0%)]\tLoss: 0.000223\n",
      "Train Epoch: 135 [0/177775 (0%)]\tLoss: 0.000220\n",
      "Train Epoch: 136 [0/177775 (0%)]\tLoss: 0.000234\n",
      "Train Epoch: 137 [0/177775 (0%)]\tLoss: 0.000282\n",
      "Train Epoch: 138 [0/177775 (0%)]\tLoss: 0.000223\n",
      "Train Epoch: 139 [0/177775 (0%)]\tLoss: 0.000223\n",
      "Train Epoch: 140 [0/177775 (0%)]\tLoss: 0.000228\n",
      "Train Epoch: 141 [0/177775 (0%)]\tLoss: 0.000233\n",
      "Train Epoch: 142 [0/177775 (0%)]\tLoss: 0.000237\n",
      "Train Epoch: 143 [0/177775 (0%)]\tLoss: 0.000231\n",
      "Train Epoch: 144 [0/177775 (0%)]\tLoss: 0.000229\n",
      "Train Epoch: 145 [0/177775 (0%)]\tLoss: 0.000226\n",
      "Train Epoch: 146 [0/177775 (0%)]\tLoss: 0.000217\n",
      "Train Epoch: 147 [0/177775 (0%)]\tLoss: 0.000234\n",
      "Train Epoch: 148 [0/177775 (0%)]\tLoss: 0.000221\n",
      "Train Epoch: 149 [0/177775 (0%)]\tLoss: 0.000355\n",
      "Train Epoch: 150 [0/177775 (0%)]\tLoss: 0.000216\n",
      "Train Epoch: 151 [0/177775 (0%)]\tLoss: 0.000219\n",
      "Train Epoch: 152 [0/177775 (0%)]\tLoss: 0.000214\n",
      "Train Epoch: 153 [0/177775 (0%)]\tLoss: 0.000212\n",
      "Train Epoch: 154 [0/177775 (0%)]\tLoss: 0.000221\n",
      "Train Epoch: 155 [0/177775 (0%)]\tLoss: 0.000210\n",
      "Train Epoch: 156 [0/177775 (0%)]\tLoss: 0.000216\n",
      "Train Epoch: 157 [0/177775 (0%)]\tLoss: 0.000233\n",
      "Train Epoch: 158 [0/177775 (0%)]\tLoss: 0.000223\n",
      "Train Epoch: 159 [0/177775 (0%)]\tLoss: 0.000221\n",
      "Train Epoch: 160 [0/177775 (0%)]\tLoss: 0.000212\n",
      "Train Epoch: 161 [0/177775 (0%)]\tLoss: 0.000230\n",
      "Train Epoch: 162 [0/177775 (0%)]\tLoss: 0.000206\n",
      "Train Epoch: 163 [0/177775 (0%)]\tLoss: 0.000215\n",
      "Train Epoch: 164 [0/177775 (0%)]\tLoss: 0.000268\n",
      "Train Epoch: 165 [0/177775 (0%)]\tLoss: 0.000201\n",
      "Train Epoch: 166 [0/177775 (0%)]\tLoss: 0.000227\n",
      "Train Epoch: 167 [0/177775 (0%)]\tLoss: 0.000210\n",
      "Train Epoch: 168 [0/177775 (0%)]\tLoss: 0.000241\n",
      "Train Epoch: 169 [0/177775 (0%)]\tLoss: 0.000206\n",
      "Train Epoch: 170 [0/177775 (0%)]\tLoss: 0.000278\n",
      "Train Epoch: 171 [0/177775 (0%)]\tLoss: 0.000226\n",
      "Train Epoch: 172 [0/177775 (0%)]\tLoss: 0.000228\n",
      "Train Epoch: 173 [0/177775 (0%)]\tLoss: 0.000207\n",
      "Train Epoch: 174 [0/177775 (0%)]\tLoss: 0.000199\n",
      "Train Epoch: 175 [0/177775 (0%)]\tLoss: 0.000203\n",
      "Train Epoch: 176 [0/177775 (0%)]\tLoss: 0.000237\n",
      "Train Epoch: 177 [0/177775 (0%)]\tLoss: 0.000194\n",
      "Train Epoch: 178 [0/177775 (0%)]\tLoss: 0.000200\n",
      "Train Epoch: 179 [0/177775 (0%)]\tLoss: 0.000188\n",
      "Train Epoch: 180 [0/177775 (0%)]\tLoss: 0.000191\n",
      "Train Epoch: 181 [0/177775 (0%)]\tLoss: 0.000191\n",
      "Train Epoch: 182 [0/177775 (0%)]\tLoss: 0.000259\n",
      "Train Epoch: 183 [0/177775 (0%)]\tLoss: 0.000191\n",
      "Train Epoch: 184 [0/177775 (0%)]\tLoss: 0.000192\n",
      "Train Epoch: 185 [0/177775 (0%)]\tLoss: 0.000187\n",
      "Train Epoch: 186 [0/177775 (0%)]\tLoss: 0.000231\n",
      "Train Epoch: 187 [0/177775 (0%)]\tLoss: 0.000184\n",
      "Train Epoch: 188 [0/177775 (0%)]\tLoss: 0.000318\n",
      "Train Epoch: 189 [0/177775 (0%)]\tLoss: 0.000183\n",
      "Train Epoch: 190 [0/177775 (0%)]\tLoss: 0.000180\n",
      "Train Epoch: 191 [0/177775 (0%)]\tLoss: 0.000179\n",
      "Train Epoch: 192 [0/177775 (0%)]\tLoss: 0.000197\n",
      "Train Epoch: 193 [0/177775 (0%)]\tLoss: 0.000176\n",
      "Train Epoch: 194 [0/177775 (0%)]\tLoss: 0.000200\n",
      "Train Epoch: 195 [0/177775 (0%)]\tLoss: 0.000182\n",
      "Train Epoch: 196 [0/177775 (0%)]\tLoss: 0.000207\n",
      "Train Epoch: 197 [0/177775 (0%)]\tLoss: 0.000170\n",
      "Train Epoch: 198 [0/177775 (0%)]\tLoss: 0.000170\n",
      "Train Epoch: 199 [0/177775 (0%)]\tLoss: 0.000193\n",
      "mean: 0.23798061907291412\n",
      "\n",
      "Test set: Average loss: 0.2380, Accuracy: 0/50000 (0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for i, (features, error) in enumerate(train_loader):\n",
    "        features, error = features.to(device), error.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = torch.nn.functional.mse_loss(output,error)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(features), len(train_loader.dataset),\n",
    "                100. * i / len(train_loader), loss))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    out = pd.DataFrame()\n",
    "    for features, error in test_loader:\n",
    "        features, error = features.to(device), error.to(device)\n",
    "        output = model(features)\n",
    "        out = out.append(pd.DataFrame(output.detach().numpy()))\n",
    "        test_loss = torch.nn.functional.mse_loss(output,error)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(error.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    # test_loss /= len(test_loader.dataset)\n",
    "    print('mean: {}'.format(test_loss))\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "       100. * correct / len(test_loader.dataset)))\n",
    "    out.to_csv('one.csv')\n",
    "\n",
    "for epoch in range(1, 200):\n",
    "    train(epoch)\n",
    "    \n",
    "test()\n",
    "# train(1)\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a63ca959-b602-4e2c-b077-d62219bd6bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strike</th>\n",
       "      <th>Tenor</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Nu</th>\n",
       "      <th>Rho</th>\n",
       "      <th>FDM_vol</th>\n",
       "      <th>Hagan_vol</th>\n",
       "      <th>Error</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.613705</td>\n",
       "      <td>7.897557</td>\n",
       "      <td>0.400207</td>\n",
       "      <td>0.527964</td>\n",
       "      <td>0.447844</td>\n",
       "      <td>0.244886</td>\n",
       "      <td>0.399076</td>\n",
       "      <td>0.465965</td>\n",
       "      <td>0.399076</td>\n",
       "      <td>0.860249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.703121</td>\n",
       "      <td>5.417643</td>\n",
       "      <td>0.843205</td>\n",
       "      <td>0.358930</td>\n",
       "      <td>0.288882</td>\n",
       "      <td>-0.256672</td>\n",
       "      <td>0.637738</td>\n",
       "      <td>0.731363</td>\n",
       "      <td>0.637738</td>\n",
       "      <td>0.858528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.946012</td>\n",
       "      <td>16.667993</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.595940</td>\n",
       "      <td>1.262982</td>\n",
       "      <td>0.046909</td>\n",
       "      <td>0.179535</td>\n",
       "      <td>0.440015</td>\n",
       "      <td>0.179535</td>\n",
       "      <td>0.831281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.521632</td>\n",
       "      <td>6.218122</td>\n",
       "      <td>0.319071</td>\n",
       "      <td>0.490090</td>\n",
       "      <td>0.223082</td>\n",
       "      <td>0.133529</td>\n",
       "      <td>0.384396</td>\n",
       "      <td>0.390709</td>\n",
       "      <td>0.384396</td>\n",
       "      <td>0.857818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.268793</td>\n",
       "      <td>16.224674</td>\n",
       "      <td>0.792654</td>\n",
       "      <td>0.152832</td>\n",
       "      <td>1.336846</td>\n",
       "      <td>-0.148154</td>\n",
       "      <td>0.246090</td>\n",
       "      <td>2.501871</td>\n",
       "      <td>0.246090</td>\n",
       "      <td>0.855235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>0.732634</td>\n",
       "      <td>13.239729</td>\n",
       "      <td>0.517281</td>\n",
       "      <td>0.119887</td>\n",
       "      <td>1.105817</td>\n",
       "      <td>-0.155785</td>\n",
       "      <td>0.287666</td>\n",
       "      <td>1.553630</td>\n",
       "      <td>0.287666</td>\n",
       "      <td>0.862545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>0.550702</td>\n",
       "      <td>3.073655</td>\n",
       "      <td>0.773149</td>\n",
       "      <td>0.337450</td>\n",
       "      <td>1.819316</td>\n",
       "      <td>0.360897</td>\n",
       "      <td>0.628061</td>\n",
       "      <td>1.777739</td>\n",
       "      <td>0.628061</td>\n",
       "      <td>0.862465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>1.882401</td>\n",
       "      <td>20.934094</td>\n",
       "      <td>0.292555</td>\n",
       "      <td>0.642293</td>\n",
       "      <td>0.907877</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.384071</td>\n",
       "      <td>0.918212</td>\n",
       "      <td>0.384071</td>\n",
       "      <td>0.862671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>1.611582</td>\n",
       "      <td>13.998001</td>\n",
       "      <td>0.107307</td>\n",
       "      <td>0.324410</td>\n",
       "      <td>1.040509</td>\n",
       "      <td>-0.189993</td>\n",
       "      <td>0.113709</td>\n",
       "      <td>0.418639</td>\n",
       "      <td>0.113709</td>\n",
       "      <td>0.861715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>1.897868</td>\n",
       "      <td>25.627190</td>\n",
       "      <td>0.522793</td>\n",
       "      <td>0.228670</td>\n",
       "      <td>0.435261</td>\n",
       "      <td>0.226394</td>\n",
       "      <td>0.314312</td>\n",
       "      <td>0.703987</td>\n",
       "      <td>0.314312</td>\n",
       "      <td>0.854430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Strike      Tenor     Alpha      Beta        Nu       Rho   FDM_vol  \\\n",
       "0      1.613705   7.897557  0.400207  0.527964  0.447844  0.244886  0.399076   \n",
       "1      1.703121   5.417643  0.843205  0.358930  0.288882 -0.256672  0.637738   \n",
       "2      0.946012  16.667993  0.130500  0.595940  1.262982  0.046909  0.179535   \n",
       "3      0.521632   6.218122  0.319071  0.490090  0.223082  0.133529  0.384396   \n",
       "4      1.268793  16.224674  0.792654  0.152832  1.336846 -0.148154  0.246090   \n",
       "...         ...        ...       ...       ...       ...       ...       ...   \n",
       "49995  0.732634  13.239729  0.517281  0.119887  1.105817 -0.155785  0.287666   \n",
       "49996  0.550702   3.073655  0.773149  0.337450  1.819316  0.360897  0.628061   \n",
       "49997  1.882401  20.934094  0.292555  0.642293  0.907877  0.002441  0.384071   \n",
       "49998  1.611582  13.998001  0.107307  0.324410  1.040509 -0.189993  0.113709   \n",
       "49999  1.897868  25.627190  0.522793  0.228670  0.435261  0.226394  0.314312   \n",
       "\n",
       "       Hagan_vol     Error         0  \n",
       "0       0.465965  0.399076  0.860249  \n",
       "1       0.731363  0.637738  0.858528  \n",
       "2       0.440015  0.179535  0.831281  \n",
       "3       0.390709  0.384396  0.857818  \n",
       "4       2.501871  0.246090  0.855235  \n",
       "...          ...       ...       ...  \n",
       "49995   1.553630  0.287666  0.862545  \n",
       "49996   1.777739  0.628061  0.862465  \n",
       "49997   0.918212  0.384071  0.862671  \n",
       "49998   0.418639  0.113709  0.861715  \n",
       "49999   0.703987  0.314312  0.854430  \n",
       "\n",
       "[50000 rows x 10 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.read_csv('one.csv', usecols=['0'])\n",
    "test_raw_data = pd.concat([test_raw_data, output],axis=1)\n",
    "test_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed004771-5945-418d-a38c-cd9cd9de1a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strike</th>\n",
       "      <th>Tenor</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Nu</th>\n",
       "      <th>Rho</th>\n",
       "      <th>FDM_vol</th>\n",
       "      <th>Hagan_vol</th>\n",
       "      <th>Error</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.613705</td>\n",
       "      <td>7.897557</td>\n",
       "      <td>0.400207</td>\n",
       "      <td>0.527964</td>\n",
       "      <td>0.447844</td>\n",
       "      <td>0.244886</td>\n",
       "      <td>0.399076</td>\n",
       "      <td>0.465965</td>\n",
       "      <td>0.399076</td>\n",
       "      <td>0.860249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.703121</td>\n",
       "      <td>5.417643</td>\n",
       "      <td>0.843205</td>\n",
       "      <td>0.358930</td>\n",
       "      <td>0.288882</td>\n",
       "      <td>-0.256672</td>\n",
       "      <td>0.637738</td>\n",
       "      <td>0.731363</td>\n",
       "      <td>0.637738</td>\n",
       "      <td>0.858528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.946012</td>\n",
       "      <td>16.667993</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.595940</td>\n",
       "      <td>1.262982</td>\n",
       "      <td>0.046909</td>\n",
       "      <td>0.179535</td>\n",
       "      <td>0.440015</td>\n",
       "      <td>0.179535</td>\n",
       "      <td>0.831281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.521632</td>\n",
       "      <td>6.218122</td>\n",
       "      <td>0.319071</td>\n",
       "      <td>0.490090</td>\n",
       "      <td>0.223082</td>\n",
       "      <td>0.133529</td>\n",
       "      <td>0.384396</td>\n",
       "      <td>0.390709</td>\n",
       "      <td>0.384396</td>\n",
       "      <td>0.857818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.268793</td>\n",
       "      <td>16.224674</td>\n",
       "      <td>0.792654</td>\n",
       "      <td>0.152832</td>\n",
       "      <td>1.336846</td>\n",
       "      <td>-0.148154</td>\n",
       "      <td>0.246090</td>\n",
       "      <td>2.501871</td>\n",
       "      <td>0.246090</td>\n",
       "      <td>0.855235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.034019</td>\n",
       "      <td>4.258776</td>\n",
       "      <td>0.376751</td>\n",
       "      <td>0.140014</td>\n",
       "      <td>1.296823</td>\n",
       "      <td>-0.091058</td>\n",
       "      <td>0.299766</td>\n",
       "      <td>0.592725</td>\n",
       "      <td>0.299766</td>\n",
       "      <td>0.849236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1.039462</td>\n",
       "      <td>14.059979</td>\n",
       "      <td>0.322589</td>\n",
       "      <td>0.229697</td>\n",
       "      <td>0.124967</td>\n",
       "      <td>0.084005</td>\n",
       "      <td>0.321435</td>\n",
       "      <td>0.335799</td>\n",
       "      <td>0.321435</td>\n",
       "      <td>0.858151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.835860</td>\n",
       "      <td>6.803596</td>\n",
       "      <td>0.446768</td>\n",
       "      <td>0.342041</td>\n",
       "      <td>1.256399</td>\n",
       "      <td>0.090066</td>\n",
       "      <td>0.320330</td>\n",
       "      <td>0.934428</td>\n",
       "      <td>0.320330</td>\n",
       "      <td>0.849959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1.320985</td>\n",
       "      <td>10.631079</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.709077</td>\n",
       "      <td>0.620432</td>\n",
       "      <td>-0.329691</td>\n",
       "      <td>0.164034</td>\n",
       "      <td>0.223488</td>\n",
       "      <td>0.164034</td>\n",
       "      <td>0.861669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1.592915</td>\n",
       "      <td>29.048153</td>\n",
       "      <td>0.668716</td>\n",
       "      <td>0.582251</td>\n",
       "      <td>0.811285</td>\n",
       "      <td>-0.023804</td>\n",
       "      <td>0.604842</td>\n",
       "      <td>1.669983</td>\n",
       "      <td>0.604842</td>\n",
       "      <td>0.859931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Strike      Tenor     Alpha      Beta        Nu       Rho   FDM_vol  \\\n",
       "0    1.613705   7.897557  0.400207  0.527964  0.447844  0.244886  0.399076   \n",
       "1    1.703121   5.417643  0.843205  0.358930  0.288882 -0.256672  0.637738   \n",
       "2    0.946012  16.667993  0.130500  0.595940  1.262982  0.046909  0.179535   \n",
       "3    0.521632   6.218122  0.319071  0.490090  0.223082  0.133529  0.384396   \n",
       "4    1.268793  16.224674  0.792654  0.152832  1.336846 -0.148154  0.246090   \n",
       "..        ...        ...       ...       ...       ...       ...       ...   \n",
       "995  1.034019   4.258776  0.376751  0.140014  1.296823 -0.091058  0.299766   \n",
       "996  1.039462  14.059979  0.322589  0.229697  0.124967  0.084005  0.321435   \n",
       "997  0.835860   6.803596  0.446768  0.342041  1.256399  0.090066  0.320330   \n",
       "998  1.320985  10.631079  0.194139  0.709077  0.620432 -0.329691  0.164034   \n",
       "999  1.592915  29.048153  0.668716  0.582251  0.811285 -0.023804  0.604842   \n",
       "\n",
       "     Hagan_vol     Error         0  \n",
       "0     0.465965  0.399076  0.860249  \n",
       "1     0.731363  0.637738  0.858528  \n",
       "2     0.440015  0.179535  0.831281  \n",
       "3     0.390709  0.384396  0.857818  \n",
       "4     2.501871  0.246090  0.855235  \n",
       "..         ...       ...       ...  \n",
       "995   0.592725  0.299766  0.849236  \n",
       "996   0.335799  0.321435  0.858151  \n",
       "997   0.934428  0.320330  0.849959  \n",
       "998   0.223488  0.164034  0.861669  \n",
       "999   1.669983  0.604842  0.859931  \n",
       "\n",
       "[1000 rows x 10 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b09be5-cb21-4ea8-bee0-1cfd683be787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
